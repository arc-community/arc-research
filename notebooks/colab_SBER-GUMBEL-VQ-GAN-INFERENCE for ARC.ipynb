{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SBER-GUMBEL-VQ-GAN-INFERENCE for ARC.ipynb","provenance":[{"file_id":"16NcbwnbRjDTwM_RweoKNbFMa3NMU4GYI","timestamp":1656863603136}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"2Vf0wK6ZFkht","executionInfo":{"status":"ok","timestamp":1656964164517,"user_tz":-120,"elapsed":3896,"user":{"displayName":"XMaster96 .de","userId":"14965015688016061310"}},"outputId":"d3e5bbb2-66ab-4855-f3d7-bd8add625147","colab":{"base_uri":"https://localhost:8080/"}},"source":["import multiprocessing\n","import torch\n","from psutil import virtual_memory\n","\n","ram_gb = round(virtual_memory().total / 1024**3, 1)\n","\n","print('CPU:', multiprocessing.cpu_count())\n","print('RAM GB:', ram_gb)\n","print(\"PyTorch version:\", torch.__version__)\n","print(\"CUDA version:\", torch.version.cuda)\n","print(\"cuDNN version:\", torch.backends.cudnn.version())\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(\"device:\", device.type)\n","\n","!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU: 2\n","RAM GB: 12.7\n","PyTorch version: 1.11.0+cu113\n","CUDA version: 11.3\n","cuDNN version: 8200\n","device: cuda\n","Mon Jul  4 19:49:23 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   51C    P8    13W /  70W |      3MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XpFUWjF-iiZ6","executionInfo":{"status":"ok","timestamp":1656964175999,"user_tz":-120,"elapsed":11489,"user":{"displayName":"XMaster96 .de","userId":"14965015688016061310"}},"outputId":"d0e98ee6-fad1-4bfc-d02e-314539e352ee"},"source":["!git clone https://github.com/CompVis/taming-transformers.git\n","!git clone https://github.com/fchollet/ARC.git"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'taming-transformers'...\n","remote: Enumerating objects: 1335, done.\u001b[K\n","remote: Total 1335 (delta 0), reused 0 (delta 0), pack-reused 1335\u001b[K\n","Receiving objects: 100% (1335/1335), 409.77 MiB | 48.37 MiB/s, done.\n","Resolving deltas: 100% (277/277), done.\n","Cloning into 'ARC'...\n","remote: Enumerating objects: 1159, done.\u001b[K\n","remote: Counting objects: 100% (3/3), done.\u001b[K\n","remote: Compressing objects: 100% (3/3), done.\u001b[K\n","remote: Total 1159 (delta 0), reused 2 (delta 0), pack-reused 1156\u001b[K\n","Receiving objects: 100% (1159/1159), 473.41 KiB | 4.30 MiB/s, done.\n","Resolving deltas: 100% (670/670), done.\n"]}]},{"cell_type":"code","source":["import os\n","os.listdir(\"./\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NDtfdMtqnHEr","executionInfo":{"status":"ok","timestamp":1656964176000,"user_tz":-120,"elapsed":9,"user":{"displayName":"XMaster96 .de","userId":"14965015688016061310"}},"outputId":"7de5f696-34e6-46df-c729-e2a37fbdf1ba"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['.config', 'ARC', 'taming-transformers', 'sample_data']"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"osx6bDfhkCzL","executionInfo":{"status":"ok","timestamp":1656964198816,"user_tz":-120,"elapsed":22822,"user":{"displayName":"XMaster96 .de","userId":"14965015688016061310"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"45e4f7aa-b05b-4fc5-a7a2-fe4fb10d4d85"},"source":["!pip install omegaconf > /dev/null\n","!pip install pytorch_lightning > /dev/null\n","!pip install einops > /dev/null\n","!pip install DALL-E > /dev/null"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"xRmR7QouCEbf"},"source":["import io\n","import os\n","import sys\n","import yaml\n","import gdown\n","import glob\n","import json\n","from math import sqrt\n","sys.path.append(\"./taming-transformers\")\n","\n","import requests\n","import numpy as np\n","import PIL\n","from PIL import Image\n","from PIL import ImageDraw, ImageFont\n","from matplotlib import pyplot as plt\n","from omegaconf import OmegaConf\n","from einops import rearrange\n","\n","import torch\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","import torchvision.transforms.functional as TF\n","\n","import torch\n","torch.set_grad_enabled(False);\n","\n","from taming.models.vqgan import VQModel, GumbelVQ"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y3WRdcOAlEqv"},"source":["models_folder = './models'\n","configs_folder = './configs'\n","\n","os.makedirs(models_folder, exist_ok=True)\n","os.makedirs(configs_folder, exist_ok=True)\n","\n","models_storage = [\n","{\n","    'id': '1COec-dpskvwHbIl9QA8qy_9nuOzsRLkl',\n","    'name': 'encoder.pkl',\n","},\n","{\n","    'id': '1pCIvZnVrzA968dqSAi2OEj299Y9YLcDQ',\n","    'name': 'decoder.pkl',\n","},\n","{\n","    'id': '1yB5nPXiJqYnoBEOannq_M5JJ2lpzhp3T',\n","    'name': 'vqgan.16384.model.ckpt',\n","},\n","{\n","    'id': '1UHuUUWX5F4y17oaW8sWuDzrsXyExU-rK',\n","    'name': 'vqgan.gumbelf8.model.ckpt',\n","},\n","{\n","    'id': '1WP6Li2Po8xYcQPGMpmaxIlI1yPB5lF5m',\n","    'name': 'sber.gumbelf8.ckpt',\n","},\n","]\n","\n","configs_storage = [{\n","    'id': '1mXu9ThC3ET_uFGPwCYKCbOXqwma7wHo-',\n","    'name': 'vqgan.16384.config.yml',\n","},{\n","    'id': '1M7RvSoiuKBwpF-98sScKng0lsZnwFebR',\n","    'name': 'vqgan.gumbelf8.config.yml',\n","}]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MnuobDm10KON"},"source":["url_template = 'https://drive.google.com/uc?id={}'\n","\n","for item in models_storage:\n","    out_name = os.path.join(models_folder, item['name'])\n","    url = url_template.format(item['id'])\n","    gdown.download(url, out_name, quiet=True)\n","\n","for item in configs_storage:\n","    out_name = os.path.join(configs_folder, item['name'])\n","    url = url_template.format(item['id'])\n","    gdown.download(url, out_name, quiet=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xAbaP_cgTuml"},"source":["def map_pixels(x, eps=0.1):\n","    return (1 - 2 * eps) * x + eps\n","\n","\n","def unmap_pixels(x, eps=0.1):\n","    return torch.clamp((x - eps) / (1 - 2 * eps), 0, 1)\n","\n","\n","class VQVAE(torch.nn.Module):\n","    def __init__(self, enc_path, dec_path):\n","        super().__init__()\n","\n","        self.enc = torch.load(enc_path, map_location=torch.device('cpu'))\n","        self.dec = torch.load(dec_path, map_location=torch.device('cpu'))\n","\n","        self.num_layers = 3\n","        self.image_size = 256\n","        self.num_tokens = 8192\n","\n","    @torch.no_grad()\n","    def get_codebook_indices(self, img):\n","        img = map_pixels(img)\n","        z_logits = self.enc.blocks(img)\n","        z = torch.argmax(z_logits, dim=1)\n","        return rearrange(z, 'b h w -> b (h w)')\n","\n","    def decode(self, img_seq):\n","        b, n = img_seq.shape\n","        img_seq = rearrange(img_seq, 'b (h w) -> b h w', h=int(sqrt(n)))\n","\n","        z = torch.nn.functional.one_hot(img_seq, num_classes=self.num_tokens)\n","        z = rearrange(z, 'b h w c -> b c h w').float()\n","        x_stats = self.dec(z).float()\n","        x_rec = unmap_pixels(torch.sigmoid(x_stats[:, :3]))\n","        return x_rec\n","\n","    def forward(self, img):\n","        raise NotImplementedError"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YP9QDodGii1y"},"source":["def download_image(url):\n","    resp = requests.get(url)\n","    resp.raise_for_status()\n","    return PIL.Image.open(io.BytesIO(resp.content))\n","\n","def load_config(config_path, display=False):\n","    config = OmegaConf.load(config_path)\n","    if display:\n","        print(yaml.dump(OmegaConf.to_container(config)))\n","    return config\n","\n","def preprocess(img, target_image_size=256, map_dalle=True):\n","    s = min(img.size)\n","        \n","    r = target_image_size / s\n","    s = (round(r * img.size[1]), round(r * img.size[0]))\n","    img = TF.resize(img, s, interpolation=PIL.Image.LANCZOS)\n","    img = TF.center_crop(img, output_size=2 * [target_image_size])\n","    img = torch.unsqueeze(T.ToTensor()(img), 0)\n","    if map_dalle: \n","        img = map_pixels(img)\n","    return img\n","\n","def load_vqgan(config, ckpt_path=None, is_gumbel=False):\n","    if is_gumbel:\n","        model = GumbelVQ(**config.model.params)\n","    else:\n","        model = VQModel(**config.model.params)\n","    if ckpt_path is not None:\n","        sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n","        missing, unexpected = model.load_state_dict(sd, strict=False)\n","    return model.eval()\n","\n","def preprocess_vqgan(x):\n","    x = 2.*x - 1.\n","    return x\n","\n","def map_pixels(x, eps=0.1):\n","    return (1 - 2 * eps) * x + eps\n","\n","def vae_postprocess(x):\n","    x = x.detach().cpu()\n","    x = torch.clamp(x, 0., 1.)\n","    x = x.permute(1,2,0).numpy()\n","    x = (255*x).astype(np.uint8)\n","    x = Image.fromarray(x)\n","    if not x.mode == \"RGB\":\n","        x = x.convert(\"RGB\")\n","    return x\n","    \n","def vqgan_postprocess(x):\n","    x = x.detach().cpu()\n","    x = torch.clamp(x, -1., 1.)\n","    x = (x + 1.)/2.\n","    x = x.permute(1,2,0).numpy()\n","    x = (255*x).astype(np.uint8)\n","    x = Image.fromarray(x)\n","    if not x.mode == \"RGB\":\n","        x = x.convert(\"RGB\")\n","    return x\n","\n","def reconstruct_with_vqgan(x, model):\n","    with torch.no_grad():\n","        z, _, [_, _, indices] = model.encode(x)\n","        #print('z:', z.shape)\n","        #print('indices:', indices.shape)\n","        #print(indices)\n","        #print(z)\n","        xrec = model.decode(z)\n","    return xrec\n","\n","def reconstruct_with_vae(x, model):\n","    with torch.no_grad():\n","        img_seq = model.get_codebook_indices(x)\n","        out_img = model.decode(img_seq)\n","    return out_img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sjPPbgKSii5_"},"source":["DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","'''\n","models_info = [{\n","    'model_name': 'VAE',\n","    'enc_path': './models/encoder.pkl',\n","    'dec_path': './models/decoder.pkl',\n","},{\n","    'model_name': '16384',\n","    'config_path': './configs/vqgan.16384.config.yml',\n","    'ckpt_path': './models/vqgan.16384.model.ckpt',\n","    'is_gumbel': False,\n","},{\n","    'model_name': 'gumbelf8',\n","    'config_path': './configs/vqgan.gumbelf8.config.yml',\n","    'ckpt_path': './models/vqgan.gumbelf8.model.ckpt',\n","    'is_gumbel': True,\n","},{\n","    'model_name': 'SBER-gumbelf8',\n","    'config_path': './configs/vqgan.gumbelf8.config.yml',\n","    'ckpt_path': './models/sber.gumbelf8.ckpt',\n","    'is_gumbel': True,\n","},]\n","'''\n","\n","models_info = [{\n","    'model_name': 'SBER-gumbelf8',\n","    'config_path': './configs/vqgan.gumbelf8.config.yml',\n","    'ckpt_path': './models/sber.gumbelf8.ckpt',\n","    'is_gumbel': True,\n","}]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z5VH-8MLii8C","executionInfo":{"status":"ok","timestamp":1656964392421,"user_tz":-120,"elapsed":23939,"user":{"displayName":"XMaster96 .de","userId":"14965015688016061310"}},"outputId":"1165cdfc-312c-44e3-a828-188be37a6a83"},"source":["\n","models = []\n","for model_info in models_info:\n","    if model_info['model_name'] == 'VAE':\n","        model = VQVAE(model_info['enc_path'], \n","                      model_info['dec_path']).eval().to(DEVICE)\n","    else:\n","        config = load_config(model_info['config_path'], display=False)\n","        model = load_vqgan(config, \n","                           ckpt_path=model_info['ckpt_path'], \n","                           is_gumbel=model_info['is_gumbel']).to(DEVICE)\n","    models.append({\n","        'model_name': model_info['model_name'],\n","        'model': model,\n","    })\n","    model = None\n","    config = None\n","    del model\n","    del config"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Working with z of shape (1, 256, 32, 32) = 262144 dimensions.\n"]}]},{"cell_type":"code","metadata":{"id":"FwggW6H9ii-Q"},"source":["\n","def stack_reconstructions(images):\n","    gt_img = images[0]['image']\n","    w, h = gt_img.size[0], gt_img.size[1]\n","    font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSans-BoldItalic.ttf\", 12)\n","    imgs_count = len(images)\n","    img = Image.new(\"RGB\", (imgs_count*w, h))\n","    \n","    for i, pr_img in enumerate(images):\n","        img.paste(pr_img['image'], (i*w, 0))\n","        ImageDraw.Draw(img).text((i*w, 0), pr_img['title'], (255, 0, 0), font=font)\n","    return img\n","\n","def reconstruction_pipeline_by_url(models, url, size=256):\n","    original_image = download_image(url)\n","    img = preprocess(original_image, target_image_size=size)\n","    images = [{\n","        'image': vae_postprocess(img[0]), \n","        'title': 'original',\n","    }]\n","    for model in models:\n","        map_dalle = model['model_name'] == 'VAE'\n","        \n","        x = preprocess(original_image, target_image_size=size, map_dalle=map_dalle)\n","        x = x.to(DEVICE)\n","        \n","        if model['model_name'] == 'VAE':\n","            pr_imgs = reconstruct_with_vae(x, model['model'])\n","            pr_img = vae_postprocess(pr_imgs[0])\n","        else:\n","            pr_imgs = reconstruct_with_vqgan(preprocess_vqgan(x), model['model'])\n","            pr_img = vqgan_postprocess(pr_imgs[0])\n","        \n","        images.append({\n","            'image': pr_img,\n","            'title': model['model_name']\n","        })\n","    \n","    img = stack_reconstructions(images)\n","    \n","    \n","    plt.figure(figsize=(20, 10))\n","    plt.imshow(img)\n","    plt.axis(False)\n","    plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["COLOR_TRANSLATOR = {0: \"000000\",\n","                    1: \"0074d9\",\n","                    2: \"ff4136\",\n","                    3: \"2ecc40\",\n","                    4: \"ffdc00\",\n","                    5: \"aaaaaa\",\n","                    6: \"f012be\",\n","                    7: \"ff851b\",\n","                    8: \"7fdbff\",\n","                    9: \"870c25\",\n","                    10: \"ffffff\"}\n","\n","COLOR_TRANSLATOR = {k: list(int(v[i:i+2], 16) for i in (0, 2, 4)) for k, v in COLOR_TRANSLATOR.items()}\n","COLOR_TRANSLATOR = {k: np.array(v) for k, v in COLOR_TRANSLATOR.items()}\n","\n","def pad_grid_and_convert(grid: np.ndarray, max_grid_size: int = 32) -> np.ndarray:\n","\n","    x_size, y_size = grid.shape\n","    new_grid = np.full(shape=[max_grid_size, max_grid_size, 3], fill_value=255, dtype=np.uint8)\n","\n","    x_offset = (max_grid_size - x_size) // 2\n","    y_offset = (max_grid_size - y_size) // 2\n","\n","    for x in range(x_size):\n","        for y in range(y_size):\n","            new_grid[x + x_offset, y + y_offset] = COLOR_TRANSLATOR[grid[x, y]]\n","\n","    return new_grid\n","\n","\n","def reconstruct_arc(models, size=256):\n","    files = list(glob.glob(\"./ARC/data/training/*.json\")) + list(glob.glob(\"./ARC/data/evaluation/*.json\"))\n","\n","    for file in files:\n","        data = json.load(open(file))\n","        img = []\n","\n","        for k in ['train', 'test']:\n","            for x in data[k]:\n","                input = np.array(x['input'])\n","                output = np.array(x['output'])\n","\n","                for _original_image in [input, output]:\n","\n","                  original_image = pad_grid_and_convert(_original_image)\n","                  original_image = np.repeat(original_image, axis=0, repeats=(size // 32))\n","                  original_image = np.repeat(original_image, axis=1, repeats=(size // 32))\n","                  original_image = Image.fromarray(original_image)\n","    \n","                  img = preprocess(original_image, target_image_size=size)\n","                  images = [{\n","                      'image': vae_postprocess(img[0]), \n","                      'title': 'original',\n","                  }]\n","                  for model in models:\n","                      map_dalle = model['model_name'] == 'VAE'\n","                      \n","                      x = preprocess(original_image, target_image_size=size, map_dalle=map_dalle)\n","                      x = x.to(DEVICE)\n","                      \n","                      if model['model_name'] == 'VAE':\n","                          pr_imgs = reconstruct_with_vae(x, model['model'])\n","                          pr_img = vae_postprocess(pr_imgs[0])\n","                      else:\n","                          pr_imgs = reconstruct_with_vqgan(preprocess_vqgan(x), model['model'])\n","                          pr_img = vqgan_postprocess(pr_imgs[0])\n","                      \n","                      images.append({\n","                          'image': pr_img,\n","                          'title': model['model_name']\n","                      })\n","                  \n","                  img = stack_reconstructions(images)\n","                  \n","                  \n","                  plt.figure(figsize=(10, 5))\n","                  plt.imshow(img)\n","                  plt.axis(False)\n","                  plt.show();"],"metadata":{"id":"uhJFFGjlnpA5"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DMcoqNAYijEk"},"source":["reconstruct_arc(models, size=256)"],"execution_count":null,"outputs":[]}]}